{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the system library\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "hate: 0.5547114323640363\n",
      "irony: 0.6247755834829443\n",
      "offensive: 0.8155092112424851\n",
      "------------------------------\n",
      "TweetEval Score: 0.6649987423631553\n"
     ]
    }
   ],
   "source": [
    "# usage: evaluaton_script.py [-h] [--tweeteval_path TWEETEVAL_PATH]\n",
    "#                            [--predictions_path PREDICTIONS_PATH] [--task TASK]\n",
    "\n",
    "# optional arguments:\n",
    "#   -h, --help: show this help message and exit\n",
    "#   --tweeteval_path: Path to TweetEval dataset\n",
    "#   --predictions_path: Path to predictions files\n",
    "#   --task: Use this to get single task detailed results\n",
    "#           (emoji|emotion|hate|irony|offensive|sentiment|stance)\n",
    "#\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "TASKS = [\n",
    "    'hate',\n",
    "    'irony',\n",
    "    'offensive']\n",
    "   \n",
    "STANCE_TASKS = [\n",
    "    'abortion',\n",
    "    'atheism',\n",
    "    'climate',\n",
    "    'feminist',\n",
    "    'hillary']\n",
    "\n",
    "def load_gold_pred(args):\n",
    "    tweeteval_path = args.tweeteval_path\n",
    "    predictions_path = args.predictions_path\n",
    "    task = args.task\n",
    "\n",
    "    if 'stance' in task:\n",
    "        gold = []\n",
    "        pred = []\n",
    "        for stance_t in STANCE_TASKS:\n",
    "            gold_path = os.path.join(tweeteval_path,task,stance_t,'test_labels.txt')\n",
    "            pred_path = os.path.join(predictions_path,task,stance_t+'.txt')\n",
    "            gold.append(open(gold_path).read().split(\"\\n\")[:-1])\n",
    "            pred.append(open(pred_path).read().split(\"\\n\")[:-1])\n",
    "        # flatten lists of lists\n",
    "        gold = [p for each_target in gold for p in each_target]\n",
    "        pred = [p for each_target in pred for p in each_target]\n",
    "    else:\n",
    "        gold_path = os.path.join(tweeteval_path,task,'test_labels.txt')\n",
    "        pred_path = os.path.join(predictions_path,task+'.txt')\n",
    "        gold = open(gold_path).read().split(\"\\n\")[:-1]\n",
    "        pred = open(pred_path).read().split(\"\\n\")[:-1]\n",
    "        \n",
    "    return gold, pred\n",
    "\n",
    "def single_task_results(args):\n",
    "    task = args.task\n",
    "    tweeteval_result = -1\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        gold, pred = load_gold_pred(args)\n",
    "        results = classification_report(gold, pred, output_dict=True)\n",
    "\n",
    "        \n",
    "        # Hate (Macro f1)\n",
    "        if 'hate' in task:\n",
    "            tweeteval_result = results['macro avg']['f1-score'] \n",
    "\n",
    "        # Irony (Irony class f1)\n",
    "        elif 'irony' in task:\n",
    "            tweeteval_result = results['1']['f1-score'] \n",
    "\n",
    "        # Offensive (Macro f1)\n",
    "        elif 'offensive' in task:\n",
    "            tweeteval_result = results['macro avg']['f1-score'] \n",
    "            \n",
    "    except Exception as ex:\n",
    "        print(f\"Issues with task {task}: {ex}\")\n",
    "        \n",
    "    return tweeteval_result, results\n",
    "\n",
    "def is_all_good(all_tweeteval_results):\n",
    "    return all([r != -1 for r in all_tweeteval_results.values()])\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='TweetEval evaluation script.')\n",
    "    \n",
    "    parser.add_argument('--tweeteval_path', default=\"./datasets/\", type=str, help='Path to TweetEval datasets')\n",
    "    parser.add_argument('--predictions_path', default=\"./predictions/\", type=str, help='Path to predictions files')\n",
    "    parser.add_argument('--task', default=\"\", type=str, help='Indicate this parameter to get single task detailed results')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.task == \"\":\n",
    "        all_tweeteval_results = {}\n",
    "        \n",
    "        # Results for each task\n",
    "        for t in TASKS:\n",
    "            args.task = t\n",
    "            all_tweeteval_results[t], _ = single_task_results(args)\n",
    "            \n",
    "        # Print results (score=-1 if some results are missing)\n",
    "        print(f\"{'-'*30}\")\n",
    "        if is_all_good(all_tweeteval_results):\n",
    "            tweeteval_final_score = sum(all_tweeteval_results.values())/len(all_tweeteval_results.values())\n",
    "        else:\n",
    "            tweeteval_final_score = -1\n",
    "        for t in TASKS:\n",
    "            # Each score\n",
    "            print(f\"{t}: {all_tweeteval_results[t]}\") \n",
    "        # Final score\n",
    "        print(f\"{'-'*30}\\nTweetEval Score: {tweeteval_final_score}\")\n",
    "        \n",
    "    else:\n",
    "        # Detailed results of one single task (--task parameter)\n",
    "        tweeteval_resut, results = single_task_results(args)\n",
    "        for k in results:\n",
    "            print(k, results[k])\n",
    "        print(f\"{'-'*30}\\nTweetEval Score ({args.task}): {tweeteval_resut}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
